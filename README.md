# A Comparative Study of Text Data Augmentation Techniques in Sentiment Analysis

## Authors

- Alex He
- Yuchuan Fu
- Jiacheng Zhang
- Yimin Zhou
  
This repository contains the code implementation for experiments on data augmentation techniques in Natural Language Processing (NLP), specifically focused on sentiment analysis. The experiment aims to compare and analyze the effectiveness of different data augmentation methods on optimizing the performance of sentiment analysis models, using a pre-trained BERT model.

## Computational Resources

The project utilizes the computational resources provided by the Lambda cloud platform. The available resources are as follows:

- GPU: 1x A10 (24 GB PCIe)
- CPU: 30 vCPUs
- RAM: 200 GiB
- Storage: 1.4 TiB SSD

These resources will be utilized for running the experiments and training the models.

Please note that the resource availability may be subject to change. For more information about the Lambda cloud platform and its resources, please refer to their documentation.


## References

- Xie, Q., Dai, Z., Hovy, E., Luong, M.T., & Le, Q.V. (2020). Unsupervised Data Augmentation for Consistency Training. arXiv preprint arXiv:1904.12848.

- Ng, N., Cho, K., & Ghassemi, M. (2020). SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 1268-1283).

- Wei, J., & Zou, K. (2019). EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks. arXiv preprint arXiv:1901.11196.

- Miao, Z., Li, Y., & Wang, X. (2021). Rotom: A meta-learned data augmentation framework for entity matching, data cleaning, text classification, and beyond. In Proceedings of the 2021 International Conference on Management of Data (pp. 1303-1316).

- Kumar, V., Glaude, H., Lichy, C. d., & Campbell, W. (2019). A Closer Look At Feature Space Data Augmentation For Few-Shot Intent Classification. arXiv preprint arXiv:1910.04176.

- Yelp Inc. (2023). Yelp Open Dataset. Retrieved from [https://www.yelp.com/dataset](https://www.yelp.com/dataset) on 2023-05-26.

- Ni, J., Li, J., & McAuley, J. (2019). Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) (pp. 188-197).

- DeVries, T., & Taylor, G.W. (2017). Dataset Augmentation in Feature Space. arXiv preprint arXiv:1702.05538.

- OpenAI. (2023). GPT-4 Technical Report. arXiv preprint arXiv:2303.08774.

- Devlin, J., Chang, M.W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805.

- OpenAI. (2021). GPT-3.5 API. Retrieved from [https://openai.com](https://openai.com) on June 17, 2023.

- Ma, E. (2019). NLP Augmentation. Retrieved from [https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug) on 2019.

- Yun, S. (2019). UDA(Unsupervised Data Augmentation) with BERT. Retrieved from [https://github.com/SanghunYun/UDA_pytorch](https://github.com/SanghunYun/UDA_pytorch) (Commit 0ba5cf8d8a6f698e19a295119f084a17dfa7a1e3).


## License

This project is licensed under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-nc-sa/4.0/).

You are free to:

- Share: copy and redistribute the material in any medium or format
- Adapt: remix, transform, and build upon the material

Under the following terms:

- Attribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.
- NonCommercial: You may not use the material for commercial purposes.
- ShareAlike: If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.

Please see the [LICENSE](LICENSE) file for more details.


